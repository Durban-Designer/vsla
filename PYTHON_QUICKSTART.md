# VSLA Python Interface - Quick Start Guide

**Variable-Shape Linear Algebra for Python Developers**

VSLA brings mathematically rigorous variable-shape tensor operations to Python, eliminating zero-padding overhead and enabling novel computational patterns impossible in traditional frameworks.

## üöÄ **What Makes VSLA Different**

Unlike NumPy, PyTorch, or TensorFlow, VSLA natively handles variable-shape tensors without padding:

```python
import vsla
import numpy as np

# Traditional approach (NumPy/PyTorch) - REQUIRES PADDING
a = np.array([1.0, 2.0, 3.0])       # shape (3,)
b = np.array([4.0, 5.0])            # shape (2,) 
# Can't add directly - need manual padding to (3,)

# VSLA approach - AUTOMATIC AMBIENT PROMOTION
a = vsla.Tensor([1.0, 2.0, 3.0])    # shape (3,)
b = vsla.Tensor([4.0, 5.0])         # shape (2,)
c = a + b                           # Works! ‚Üí [5.0, 7.0, 3.0]
```

## üì¶ **Installation**

### Prerequisites
- Python 3.8+
- CMake 3.16+
- C++ compiler with C++17 support
- NumPy

### Install from Source
```bash
git clone https://github.com/royce-birnbaum/vsla.git
cd vsla/python
pip install -e .
```

### Verify Installation
```python
import vsla
print(f"VSLA version: {vsla.__version__}")
print(f"C extension available: {vsla._has_core}")
```

## üî• **Core Concepts**

### 1. **Equivalence Classes & Ambient Promotion**
VSLA treats tensors as equivalence classes where `(d‚ÇÅ,v) ~ (d‚ÇÇ,w)` if they're equal after padding to the maximum dimensions:

```python
import vsla

# These are mathematically equivalent in VSLA
a = vsla.Tensor([1, 2])      # Represents [1, 2, 0, 0, ...]
b = vsla.Tensor([1, 2, 0])   # Represents [1, 2, 0, 0, ...]

# Operations use ambient promotion automatically
c = vsla.Tensor([3, 4, 5])
result = a + c               # [1,2] + [3,4,5] ‚Üí [4,6,5]
```

### 2. **Dual Semiring Models**

**Model A (Convolution Semiring)**
```python
# For signal processing, ML convolutions
a = vsla.Tensor([1, 2, 3], model='A')
b = vsla.Tensor([0.5, 0.5], model='A')
conv_result = a.convolve(b)  # FFT-accelerated convolution
```

**Model B (Kronecker Semiring)**
```python
# For tensor networks, quantum computing
a = vsla.Tensor([1, 2], model='B')
b = vsla.Tensor([3, 4], model='B')  
kron_result = a.kronecker(b)  # [3, 4, 6, 8]
```

### 3. **Stacking Operators**
Build higher-dimensional tensors efficiently:

```python
# Basic stacking
tensors = [vsla.Tensor([1, 2]), vsla.Tensor([3, 4, 5])]
stacked = vsla.stack(tensors)  # Shape: [2, 3] with ambient promotion

# Window stacking for streaming data
window = vsla.Window(size=3)
for data in streaming_source:
    result = window.push(vsla.Tensor(data))
    if result:  # Returns stacked window when full
        process(result)

# Pyramid stacking for multi-resolution analysis  
pyramid = vsla.Pyramid(levels=4, window_size=8)
multi_res = pyramid.push(large_tensor)
```

## üìä **Performance Advantages**

### Memory Efficiency
VSLA eliminates padding overhead:

```python
import numpy as np
import vsla

# Traditional approach wastes memory on padding
sequences = [[1,2], [1,2,3], [1,2,3,4]]
padded = np.array([seq + [0]*(4-len(seq)) for seq in sequences])  # 67% waste

# VSLA approach uses exact memory
vsla_tensors = [vsla.Tensor(seq) for seq in sequences]  # 0% waste
```

### Computational Efficiency
No wasted operations on padding zeros:

```python
# Benchmarking shows 30-60% efficiency improvements
# See benchmarks/python/ for comprehensive comparisons
```

## üß™ **Common Patterns**

### Machine Learning Operations
```python
import vsla

# Variable-length sequence processing
def process_sequences(sequences):
    tensors = [vsla.Tensor(seq) for seq in sequences]
    
    # Automatic ambient promotion for batch operations
    batch_sum = sum(tensors)
    
    # Element-wise operations work across variable shapes
    normalized = [t / batch_sum for t in tensors]
    
    return normalized

# Attention mechanisms with variable context
def variable_attention(queries, keys, values):
    # All tensors can have different sequence lengths
    # VSLA handles ambient promotion automatically
    scores = [q @ k.T for q, k in zip(queries, keys)]
    attention = [softmax(s) @ v for s, v in zip(scores, values)]
    return attention
```

### Signal Processing
```python
# Convolution with variable-length signals and kernels
signals = [vsla.Tensor(sig) for sig in variable_length_signals]
kernel = vsla.Tensor(filter_coefficients)

filtered = [sig.convolve(kernel) for sig in signals]  # No padding needed!
```

### Ragged Tensor Operations
```python
# Unlike tf.RaggedTensor or torch.nested, VSLA operations are mathematically principled
ragged_data = [[1, 2], [3, 4, 5, 6], [7]]
tensors = [vsla.Tensor(row) for row in ragged_data]

# All operations work with automatic ambient promotion
row_sums = [t.sum() for t in tensors]
normalized = [t / t.sum() for t in tensors]
stacked = vsla.stack(tensors)  # Shape: [3, 4] with trailing zeros
```

## üîß **API Reference**

### Core Tensor Class
```python
class Tensor:
    def __init__(self, data, model='A', dtype='float64')
    def __add__(self, other) -> Tensor        # Ambient promotion addition
    def __mul__(self, other) -> Tensor        # Element-wise multiplication  
    def convolve(self, other) -> Tensor       # Model A: convolution
    def kronecker(self, other) -> Tensor      # Model B: Kronecker product
    def sum(self) -> float                    # Tensor sum
    def norm(self) -> float                   # L2 norm
    def shape(self) -> tuple                  # Current shape
    def to_numpy(self) -> ndarray            # Convert to NumPy
```

### Utility Functions
```python
def stack(tensors: List[Tensor]) -> Tensor
def ambient_shape(*tensors) -> tuple
def is_equivalent(a: Tensor, b: Tensor) -> bool
```

### Streaming Operations
```python
class Window:
    def __init__(self, size: int, rank: int = 1)
    def push(self, tensor: Tensor) -> Optional[Tensor]

class Pyramid:  
    def __init__(self, levels: int, window_size: int)
    def push(self, tensor: Tensor) -> Optional[Tensor]
    def flush(self) -> List[Tensor]
```

## ‚ö° **Performance Tips**

1. **Use appropriate models**: Model A for convolutions, Model B for tensor products
2. **Batch operations**: Group variable-shape operations when possible  
3. **Memory management**: VSLA uses reference counting - don't worry about copies
4. **Single-threaded**: Current Python interface is single-threaded (multi-threading planned)

## üêõ **Current Limitations**

- **1D tensors only**: Multi-dimensional support in development
- **Single-threaded**: GPU and multi-threading support planned
- **Memory debugging**: Some edge cases in development

## üìö **Examples & Benchmarks**

See the `examples/python/` directory for:
- Basic usage examples
- Performance benchmarks vs NumPy/PyTorch
- Machine learning applications
- Signal processing workflows

## ü§ù **Contributing**

VSLA is research-grade software. Contributions welcome:
- Bug reports and fixes
- Performance benchmarks
- Documentation improvements
- New applications and use cases

## üìñ **Further Reading**

- **Mathematical Foundation**: `docs/vsla_spec_v_3.2.md`
- **Research Paper**: `docs/papers/src/`
- **C API Documentation**: `include/vsla/`
- **Transformer Analysis**: `docs/1B_TRANSFORMER_PLAN.md`

---

**VSLA: Where mathematical rigor meets practical performance**