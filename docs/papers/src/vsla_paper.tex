% ====================================================================
%  Main Document - Variable‑Shape Linear Algebra
% ====================================================================
\documentclass[11pt]{article}

% --------------------------------------------------------------------
%  Preamble
% --------------------------------------------------------------------
\input{preamble.tex}

% --------------------------------------------------------------------
%  Title information
% --------------------------------------------------------------------
\title{Variable‑Shape Linear Algebra: Mathematical Foundations and High-Performance Implementation}
\author{Royce Birnbaum}
\date{July 19, 2025}

% ====================================================================
\begin{document}
\maketitle

% ================================================================
%  Abstract
% ================================================================
\begin{abstract}
Variable‑Shape Linear Algebra (VSLA) treats dimension as intrinsic data, enabling automatic shape promotion while preserving algebraic structure. This paper formalizes VSLA through equivalence classes of finite‑dimensional vectors and develops two complete semiring instantiations: convolution and Kronecker products. We introduce the stacking operator $\Stack$ that builds higher-rank tensors from variable-shape collections, forming tensor pyramids for streaming applications. The mathematical foundation builds on classical polynomial ring theory and formal power series \cite{VonZurGathenGerhard2013}, while the dual semiring approach extends traditional matrix algebra \cite{HornJohnson2012,GondranMinoux2008}. While Cheng's Semi-Tensor Product (STP) first generalized matrix products for mismatched dimensions \cite{Cheng2001}, VSLA offers a distinct, complementary approach founded on equivalence classes and dual semiring models, leading to a sparse-by-design memory model. Complexity analysis shows FFT convolution preserves $\mathcal{O}(mn d_{\max} \log d_{\max})$ efficiency using established fast multiplication algorithms \cite{CooleyTukey1965}. Unlike existing ragged tensor frameworks \cite{TF2024,PyTorch2023,NestedTensorPyTorch2022}, VSLA provides mathematically rigorous structures with provable algebraic identities, enabling principled computation for adaptive AI architectures, sensor fusion, and scientific computing.
\end{abstract}

\vspace{0.5em}
\noindent\keywords{Variable‑shape tensors, stacking operator, tensor pyramids, semiring algebra, automatic differentiation, high‑performance computing, adaptive neural networks, sensor fusion, sparse computing, Semi-Tensor Product}

\vspace{1em}
\noindent\msc{15A69, 68W30, 65F05, 16Y60}

% ================================================================
%  Sections
% ================================================================

\input{sections/introduction.tex}
\input{sections/foundations.tex}
\input{sections/model_a.tex}
\input{sections/model_b.tex}
\input{sections/stacking.tex}
\input{sections/vsla_ops.tex}
\input{sections/implementation.tex}
\input{sections/advanced_ops.tex}
\input{sections/evaluation.tex}
\input{sections/related_work.tex}
\input{sections/gradients.tex}
\input{sections/applications.tex}
\input{sections/future_work.tex}
\input{sections/conclusion.tex}

% ================================================================
%  Bibliography
% ================================================================
\bibliographystyle{apalike}
\bibliography{bibliography}

\end{document}